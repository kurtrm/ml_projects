{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "SVMs are very popular in machine learning due to their versatile capabilities in classification. They utilize _large margin classification_ which basically means to separate classes by the widest margin possible.\n",
    "\n",
    "Aurelien notes that SVMs are sensitive to feature scales, so it is advisable to scale them using either a NormalScaler or StandardScaler.\n",
    "\n",
    "We can also implement SVMs using _hard margin classification_, but the data must be linearly separable and it's very sensitive to outliers. Thus, we prefer more flexible models, ones that will keep the street as wide as possible and limit the number of margin violations. Here, we arrive at _soft margin classification_.\n",
    "\n",
    "The hyperparameter that controls this balance is **C**. We can reduce **C** if our model is overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression does not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "You can use the regular old SVC kernel and set its kernel=\"linear\" and C=1, but apparently this is much slower. \n",
    "\n",
    "Using SGDClassifer(loss=\"hinge\", alpha=1/(m*C)) will use SGD to train a linear SVM classifier. This is useful for large datasets\n",
    "that don't fit in memory.\n",
    "\n",
    "Aurelien makes further suggestions on how to improve the use of this classifier in a note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nonlinear SVM Classification\n",
    "\n",
    "Much like regression, SVMs don't have to rely on linear separability. In fact, most datasets won't be linearly separable. Additionally, low numbers of features may not reveal separations, so we can use PolynomialFeatures just as in polynomial regression to add these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('smv_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_smv_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"smv_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "])\n",
    "\n",
    "polynomial_smv_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a polynomial kernel in the SVC class that will perform the same polynomial operation as PolynomialFeatures, but it's significantly more performant. This is known as the _kernel trick_. This is ideal to use since SVCs don't deal well with low polynomial degrees and higher polynomial degrees slow the model performance.\n",
    "\n",
    "To reduced overfitting, reduce the number of polynomial degrees or if its underfitting, increase them. The coef0 parameter affects how the model is influenced by high degree polynomials versus low degree. \n",
    "\n",
    "Use grid search to find the best hyperparameters, starting with a coarse search then refining it so you get into the right space where you understand how each is playing into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another technique we can use is a similarity function which measures how much each instance resembles a landmark (so one particular instance). We can define what our similarity function is that the similarity function will use to make its determination, and in this case we'll use a _radial basis function_. I can't find a very intuitive explanation on the internet, but from trying to put two and two together, I think it will  make its similarity decision based on how far it is from the landmark, if the Guassian radial basis function is centered on it. So if it's within _n_ standard deviations or something, it will classify it as _some class_.\n",
    "\n",
    "He defines this function, but I do not think it presents any value to replicate it here.\n",
    "\n",
    "The simplest way to do this is to add a lndmark at the location of each and every instance in the dataset. That way we maximize the number of dimensions and increase the chance that the data will become linearly separable. But thi sbecomes very computationally expensive. A training set with _m_ instances and _n_ features becomes a set with _m x m_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
